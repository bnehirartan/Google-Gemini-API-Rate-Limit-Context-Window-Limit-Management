{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnehirartan/Google-Gemini-API-Rate-Limit-Context-Window-Limit-Management/blob/main/gemini-api-ratelimit-contextsize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Required Libraries"
      ],
      "metadata": {
        "id": "eYfjVH4qo4p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  !pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "o2-6m8aDsGcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1lUezp4XNd1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_markdown(text): # function converts plain text from the LLM model to Markdown format, adding blockquote styling and converting bullet points.\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "ymh_7t_lXZt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use API Key and Generative AI Models"
      ],
      "metadata": {
        "id": "3meU2gSJrs4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GEMINIAPI') # using the API key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "Nt9c-0ogeYvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model=genai.GenerativeModel('gemini-1.5-pro-latest') # initialize the generative model with the latest Gemini-1.5-Pro version"
      ],
      "metadata": {
        "id": "g_5OrRaKesJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token Counting Methods"
      ],
      "metadata": {
        "id": "imzPIqZ5pZMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(text):\n",
        "    \"\"\"\n",
        "    Counts the number of tokens in a given text using the Gemini 1.5 Pro model.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The input text whose tokens need to be counted.\n",
        "\n",
        "    Returns:\n",
        "    int: The total number of tokens in the provided text.\n",
        "    \"\"\"\n",
        "    response = model.count_tokens(text) # use the model's built-in token counting function to analyze the text\n",
        "    return response.total_tokens"
      ],
      "metadata": {
        "id": "SUenZPZJe3GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_token_usage(prompt, response_text):\n",
        "    \"\"\"\n",
        "    Calculates the token usage for a given prompt and response.\n",
        "\n",
        "    Parameters:\n",
        "    prompt (str): The input text provided to the model.\n",
        "    response_text (str): The generated response from the model.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the number of input tokens, output tokens, and total tokens used.\n",
        "    \"\"\"\n",
        "    input_tokens = count_tokens(prompt)\n",
        "    output_tokens = count_tokens(response_text)\n",
        "    total_tokens = input_tokens + output_tokens\n",
        "\n",
        "    return input_tokens, output_tokens, total_tokens"
      ],
      "metadata": {
        "id": "u9gfWlUdfM0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling API Rate Limit Method"
      ],
      "metadata": {
        "id": "VcpliA2Kp10G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using global constants instead of passing them as arguments\n",
        "MAX_TOKENS = 500\n",
        "CONTEXT_WINDOW = 1000\n",
        "WARNING_THRESHOLD = 0.8\n",
        "#RPM = 2\n",
        "#TPM = 32000"
      ],
      "metadata": {
        "id": "5RaUIi51fzee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def api_request_with_retry(request_func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Makes an API request with automatic retry logic in case of failures.\n",
        "\n",
        "    Parameters:\n",
        "    request_func (function): The API request function to be executed.\n",
        "    *args: Positional arguments to pass to the request function.\n",
        "    **kwargs: Keyword arguments to pass to the request function.\n",
        "\n",
        "    Returns:\n",
        "    Any: The response from the API request if successful, otherwise None.\n",
        "    \"\"\"\n",
        "    retries = 1\n",
        "    max_retries = 3\n",
        "    api_error_shown = False  # track if the error message was already printed (fixing the repeated API error message)*\n",
        "\n",
        "\n",
        "    while retries <= max_retries:  # condition to allow 3 retries\n",
        "        try:\n",
        "            return request_func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            error_message = str(e).lower()\n",
        "\n",
        "            if \"429\" in error_message:  # check for rate limit error\n",
        "                if not api_error_shown:\n",
        "                    print(f\"⚠ API Error Message: {e}\")  # print only once*\n",
        "                    api_error_shown = True\n",
        "\n",
        "                wait_time = 2 ** retries  # exponential backoff logic\n",
        "                print(f\"⚠️ Rate limit exceeded! Waiting for {wait_time} seconds... ({retries}/{max_retries})\")\n",
        "                time.sleep(wait_time)\n",
        "                retries += 1\n",
        "            else:\n",
        "                # if it's not a rate limit error, print a general error message and exit\n",
        "                print(\"❌ Error: API request is failed.\")\n",
        "                return None\n",
        "    # if we reach this point, all retries have failed\n",
        "    print(\"❌ Maximum number of retries reached. API request failed.\")\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "v8gECN3of21E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Context Window - Text Generation Method"
      ],
      "metadata": {
        "id": "94g6WkmGqNSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt):\n",
        "    \"\"\"\n",
        "    Generates text based on the given prompt while handling token limits and ensuring sentence completeness.\n",
        "\n",
        "    Parameters:\n",
        "    prompt (str): The input text that serves as the basis for text generation.\n",
        "\n",
        "    Returns:\n",
        "    str or None: The generated text if successful, otherwise None.\n",
        "    \"\"\"\n",
        "\n",
        "    token_count = count_tokens(prompt)\n",
        "    remaining_tokens = CONTEXT_WINDOW - token_count #calculates how many tokens are left before hitting the context window limit\n",
        "\n",
        "    if token_count >= CONTEXT_WINDOW: # checking the number of token vs context window size\n",
        "        print(\"⚠️ Warning: Prompt exceeds context window limit!\")\n",
        "        return None\n",
        "\n",
        "    if token_count + MAX_TOKENS > CONTEXT_WINDOW * WARNING_THRESHOLD: # checking the warning THRESHOLD value\n",
        "        print(\"⚠️ Warning: Token usage is close to the limit! Consider shortening input.\")\n",
        "\n",
        "    try:\n",
        "        response = api_request_with_retry(model.generate_content, prompt, generation_config={\"max_output_tokens\": min(MAX_TOKENS, remaining_tokens)})\n",
        "        if response:\n",
        "            text = response.text.strip() #remove the unnecessary whitespaces in the text (at the start and the end)\n",
        "\n",
        "            #STOP CHECK\n",
        "            if text[-1] not in [\".\", \"!\", \"?\"]:  # check if the last character is a sentence-ending punctuation\n",
        "                #print(\"Sentence cut off, requesting continuation...\") #the response was cut-off mid-sentence due to token\n",
        "\n",
        "                continuation = api_request_with_retry(   # requesting a continuation to complete the last sentence\n",
        "                    model.generate_content,\n",
        "                    \"Continue from: \" + text[-50:],  # take the last 50 characters to ensure continuity\n",
        "                    generation_config={\"max_output_tokens\": min(MAX_TOKENS, remaining_tokens)}\n",
        "                )\n",
        "                if continuation:\n",
        "                    text += \" \" + continuation.text.strip()  # if continuation is received, append it to the response\n",
        "\n",
        "            return text\n",
        "    except Exception as e:\n",
        "      # handle any unexpected error that occur during text generation\n",
        "        print(f\"❌ Error generating text: {e}\")\n",
        "    return None #indicate failure"
      ],
      "metadata": {
        "id": "nn5JgC7lgYsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Context Window - Chat Mode Method"
      ],
      "metadata": {
        "id": "8EVzNeDhqffX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def chat_mode():\n",
        "    \"\"\"\n",
        "    Initiates an interactive chat session where user inputs are processed, and AI-generated responses\n",
        "    are displayed while maintaining chat history.\n",
        "\n",
        "    The function ensures token limits are not exceeded and manages chat history accordingly.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    chat_history=[]\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Chat ended.\")\n",
        "            break\n",
        "\n",
        "        #add the user to the chat history\n",
        "        chat_history.append(user_input)\n",
        "        #combine the chat history into single string\n",
        "        context = \"\\n\".join(chat_history)\n",
        "        #count the number of tokens used in the context\n",
        "        input_tokens = count_tokens(context)\n",
        "        #calculate remaining tokens available\n",
        "        remaining_tokens = CONTEXT_WINDOW - input_tokens\n",
        "\n",
        "        #check if the context window limit is exceeded\n",
        "        if input_tokens >= CONTEXT_WINDOW:\n",
        "            print(\"⚠️ Warning: Context window limit exceeded! Consider clearing history.\")\n",
        "            print(\"🔄 Clearing the history...\")\n",
        "            #chat_history = []  # clear the history to continue chatting when the limit is exceeded\n",
        "            chat_history.pop(0)\n",
        "            context = \"\\n\".join(chat_history)\n",
        "            continue  # skip the current iteration and start fresh\n",
        "\n",
        "        if input_tokens + MAX_TOKENS > CONTEXT_WINDOW * WARNING_THRESHOLD:\n",
        "            print(\"⚠️ Warning: Token usage is close to the limit!\")\n",
        "\n",
        "        try:\n",
        "            # generate a response based on the current chat context\n",
        "            response = api_request_with_retry(\n",
        "                model.generate_content,\n",
        "                context,\n",
        "                generation_config={\"max_output_tokens\": min(MAX_TOKENS, remaining_tokens)}\n",
        "            )\n",
        "\n",
        "            # if a response is successfully generated\n",
        "            if response:\n",
        "                response_text = response.text.strip()\n",
        "\n",
        "                #STOP CHECK\n",
        "                if response_text[-1] not in [\".\", \"!\", \"?\"]:\n",
        "                    # print(\"Chat response cut off, requesting continuation...\")\n",
        "\n",
        "                    continuation = api_request_with_retry(\n",
        "                        model.generate_content,\n",
        "                        \"Continue from: \" + response_text[-50:],  # maintain context with the last 50 characters\n",
        "                        generation_config={\"max_output_tokens\": min(MAX_TOKENS, remaining_tokens)}\n",
        "                    )\n",
        "\n",
        "\n",
        "                    if continuation:\n",
        "                        response_text += \" \" + continuation.text.strip()\n",
        "\n",
        "                # add AI response to chat history\n",
        "                chat_history.append(response_text)\n",
        "\n",
        "                # calculate token usage for the input and response\n",
        "                input_tokens, output_tokens, total_tokens = get_token_usage(context, response_text)\n",
        "\n",
        "                # display token usage statistics\n",
        "                print(f\"📌 Input Tokens: {input_tokens}, Output Tokens: {output_tokens}, Total Tokens: {total_tokens}\")\n",
        "\n",
        "                # print AI-generated response\n",
        "                print(f\"AI: {response_text}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # handle any unexpected error that occur during chat mode\n",
        "            print(f\"❌ Error generating response: {e}\")\n",
        "            continue  # continue the chat loop despite the error"
      ],
      "metadata": {
        "id": "CxZkViIhiRQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_mode()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 922
        },
        "id": "i9sYT7VgkKGN",
        "outputId": "a7005c3c-dae7-49e1-817a-cdc4d7ed94b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hello\n",
            "📌 Input Tokens: 1, Output Tokens: 10, Total Tokens: 11\n",
            "AI: Hello there! How can I help you today?\n",
            "You: can you give me an information about API\n",
            "📌 Input Tokens: 21, Output Tokens: 803, Total Tokens: 824\n",
            "AI: API stands for **Application Programming Interface**.  Think of it as a messenger that allows different software systems to talk to each other and exchange information.  It defines how software components should interact, allowing developers to leverage functionalities of existing applications without needing to know the complex inner workings.\n",
            "\n",
            "Here's a breakdown of key information about APIs:\n",
            "\n",
            "**What APIs do:**\n",
            "\n",
            "* **Enable communication:** APIs act as intermediaries, facilitating the transfer of data and requests between different software systems.  They provide a standardized way for applications to communicate, regardless of their underlying technology.\n",
            "* **Expose functionality:** APIs allow developers to access specific features or data of an application without needing to understand the entire codebase. This promotes code reusability and saves development time.\n",
            "* **Connect services:** APIs are crucial for integrating different services and platforms. For example, a weather app might use a weather service's API to display current conditions, or a social media app might use a payment gateway's API to process transactions.\n",
            "\n",
            "**Key Concepts:**\n",
            "\n",
            "* **Request:**  When a program wants information or wants to perform an action, it sends a request to the API. This request specifies what it needs.\n",
            "* **Response:** The API processes the request and sends back a response, which typically contains the requested data or a confirmation of the action performed.\n",
            "* **Endpoint:**  A specific URL that handles a particular type of request. Think of it like a specific door on a building, each leading to a different room (functionality).\n",
            "* **API Key:** A unique code used to identify and authenticate the user or application making the request. This helps track usage and control access.\n",
            "* **Documentation:** Essential for developers to understand how to use the API.  It describes the available endpoints, request parameters, response formats, and other important details.\n",
            "\n",
            "**Types of APIs:**\n",
            "\n",
            "* **REST (Representational State Transfer):**  The most common type of API, using standard HTTP methods (GET, POST, PUT, DELETE) to interact with resources.\n",
            "* **SOAP (Simple Object Access Protocol):**  A more complex and structured API approach often used in enterprise applications.\n",
            "* **GraphQL:**  A newer query language that allows clients to specify exactly the data they need, improving efficiency.\n",
            "\n",
            "**Examples of APIs:**\n",
            "\n",
            "* **Google Maps API:** Allows developers to integrate maps and location services into their applications.\n",
            "* **Twitter API:**  Provides access to Twitter * **Stripe API:** Enables online payments and other financial transactions.\n",
            "* **Twilio API:**  Facilitates communication services like SMS, voice calls, and video.\n",
            "* **Google Maps API:** Offers location-based services, mapping, and directions.\n",
            "* **YouTube Data API:** Allows access to YouTube data, including videos, playlists, and channels.\n",
            "* **Spotify API:**  Provides access to Spotify's music catalog and user data.\n",
            "* **Instagram API:**  Allows access to Instagram data, including user profiles, media, and comments (with restrictions).\n",
            "* **Facebook Graph API:**  Enables interaction with Facebook's social graph, including user profiles, pages, and groups.\n",
            "* **Slack API:**  Allows integration with Slack for messaging and other functionalities.\n",
            "* **GitHub API:**  Provides access to GitHub repositories, issues, and other data.\n",
            "* **OpenWeatherMap API:** Offers current and forecast weather data.\n",
            "* **News API:**  Provides access to news articles from various sources.\n",
            "* **Cloud Vision API (Google):** Enables image analysis and recognition.\n",
            "* **Cloud Natural Language API (Google):** Provides natural language processing capabilities.\n",
            "* **Amazon S3 API:**  Allows access to Amazon S3 for object storage.\n",
            "\n",
            "\n",
            "This list is not exhaustive, but it represents a good starting point for exploring common and useful APIs.  There are thousands of APIs available, catering to a wide range of functionalities and data sources.\n",
            "You: exit\n",
            "Chat ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_mode()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qItusdNBkNRL",
        "outputId": "b85624a3-658f-40cb-defe-a3b6db2d2b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: quit\n",
            "Chat ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Request Testing (Text Generation)"
      ],
      "metadata": {
        "id": "pvHcbi6SrFxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(50):  # simulating a long conversation\n",
        "    print(f\"Test Message {i+1}\")\n",
        "    response = generate_text(f\"This is message {i+1} in a long conversation.\")\n",
        "\n",
        "    if not response:\n",
        "        print(\"❌ Test Failed: No Response Generated!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "e_mEZh-kl_Uk",
        "outputId": "81cb6a10-5023-40f3-9a4a-765cee459a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Message 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 686.47ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠ API Error Message: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).\n",
            "⚠️ Rate limit exceeded! Waiting for 2 seconds... (1/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 406.32ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Rate limit exceeded! Waiting for 4 seconds... (2/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 813.97ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Rate limit exceeded! Waiting for 8 seconds... (3/3)\n",
            "❌ Maximum number of retries reached. API request failed.\n",
            "❌ Test Failed: No Response Generated!\n"
          ]
        }
      ]
    }
  ]
}