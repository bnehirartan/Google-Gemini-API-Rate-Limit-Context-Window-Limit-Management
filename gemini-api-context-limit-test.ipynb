{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnehirartan/Google-Gemini-API-Rate-Limit-Context-Window-Limit-Management/blob/main/gemini-api-context-limit-test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Required Libraries"
      ],
      "metadata": {
        "id": "eYfjVH4qo4p6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1lUezp4XNd1"
      },
      "outputs": [],
      "source": [
        "#import numpy as np\n",
        "#from tqdm import tqdm\n",
        "#import pathlib\n",
        "import os\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_markdown(text): # function converts plain text from the LLM model to Markdown format, adding blockquote styling and converting bullet points.\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "ymh_7t_lXZt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use API Key and Generative AI Models"
      ],
      "metadata": {
        "id": "3meU2gSJrs4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GEMGEMINI_API_KEY') # using the API key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "Nt9c-0ogeYvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=genai.GenerativeModel('gemini-1.5-pro-latest') # Initialize the generative model with the latest Gemini-1.5-Pro version"
      ],
      "metadata": {
        "id": "g_5OrRaKesJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token Counting Methods"
      ],
      "metadata": {
        "id": "imzPIqZ5pZMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(text):\n",
        "    \"\"\"\n",
        "    Counts the number of tokens in a given text using the Gemini 1.5 Pro model.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The input text whose tokens need to be counted.\n",
        "\n",
        "    Returns:\n",
        "    int: The total number of tokens in the provided text.\n",
        "    \"\"\"\n",
        "    response = model.count_tokens(text) # Use the model's built-in token counting function to analyze the text\n",
        "    return response.total_tokens"
      ],
      "metadata": {
        "id": "SUenZPZJe3GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_token_usage(prompt, response_text):\n",
        "    \"\"\"\n",
        "    Calculates the token usage for a given prompt and response.\n",
        "\n",
        "    Parameters:\n",
        "    prompt (str): The input text provided to the model.\n",
        "    response_text (str): The generated response from the model.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the number of input tokens, output tokens, and total tokens used.\n",
        "    \"\"\"\n",
        "    input_tokens = count_tokens(prompt)\n",
        "    output_tokens = count_tokens(response_text)\n",
        "    total_tokens = input_tokens + output_tokens\n",
        "\n",
        "    return input_tokens, output_tokens, total_tokens"
      ],
      "metadata": {
        "id": "u9gfWlUdfM0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling API Rate Limit Method"
      ],
      "metadata": {
        "id": "VcpliA2Kp10G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use global constants instead of passing them as arguments\n",
        "MAX_TOKENS = 500\n",
        "CONTEXT_WINDOW = 1000\n",
        "WARNING_THRESHOLD = 0.8\n",
        "#RPM = 2\n",
        "#TPM = 32000"
      ],
      "metadata": {
        "id": "5RaUIi51fzee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def api_request_with_retry(request_func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Makes an API request with automatic retry logic in case of failures.\n",
        "\n",
        "    Parameters:\n",
        "    request_func (function): The API request function to be executed.\n",
        "    *args: Positional arguments to pass to the request function.\n",
        "    **kwargs: Keyword arguments to pass to the request function.\n",
        "\n",
        "    Returns:\n",
        "    Any: The response from the API request if successful, otherwise None.\n",
        "    \"\"\"\n",
        "    retries = 1\n",
        "    max_retries = 3\n",
        "    api_error_shown = False  # track if the error message was already printed (fixing the repeated API error message)*\n",
        "\n",
        "\n",
        "    while retries <= max_retries:  # Condition to allow 3 retries\n",
        "        try:\n",
        "            return request_func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            error_message = str(e).lower()\n",
        "\n",
        "            if \"429\" in error_message:  # Check for rate limit error\n",
        "                if not api_error_shown:\n",
        "                    print(f\"⚠ API Error Message: {e}\")  # Print only once*\n",
        "                    api_error_shown = True\n",
        "\n",
        "                wait_time = 2 ** retries  # Exponential backoff logic\n",
        "                print(f\"⚠️ Rate limit exceeded! Waiting for {wait_time} seconds... ({retries}/{max_retries})\")\n",
        "                time.sleep(wait_time)\n",
        "                retries += 1\n",
        "            else:\n",
        "                # If it's not a rate limit error, print a general error message and exit\n",
        "                print(\"❌ Error: API request is failed.\")\n",
        "                return None\n",
        "    # If we reach this point, all retries have failed\n",
        "    print(\"❌ Maximum number of retries reached. API request failed.\")\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "v8gECN3of21E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Context Window - Text Generation Method"
      ],
      "metadata": {
        "id": "94g6WkmGqNSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt):\n",
        "    \"\"\"\n",
        "    Generates text based on the given prompt while handling token limits and ensuring sentence completeness.\n",
        "\n",
        "    Parameters:\n",
        "    prompt (str): The input text that serves as the basis for text generation.\n",
        "\n",
        "    Returns:\n",
        "    str or None: The generated text if successful, otherwise None.\n",
        "    \"\"\"\n",
        "\n",
        "    token_count = count_tokens(prompt)\n",
        "    remaining_tokens = CONTEXT_WINDOW - token_count #calculates how many tokens are left before hitting the context window limit\n",
        "\n",
        "    if token_count >= CONTEXT_WINDOW: # checking the number of token vs context window size\n",
        "        print(\"⚠️ Warning: Prompt exceeds context window limit!\")\n",
        "        return None\n",
        "\n",
        "    if token_count + MAX_TOKENS > CONTEXT_WINDOW * WARNING_THRESHOLD: # checking the warning THRESHOLD value\n",
        "        print(\"⚠️ Warning: Token usage is close to the limit! Consider shortening input.\")\n",
        "\n",
        "    try:\n",
        "        response = api_request_with_retry(model.generate_content, prompt, generation_config={\"max_output_tokens\": min(MAX_TOKENS, remaining_tokens)})\n",
        "        if response:\n",
        "            text = response.text.strip()#deletes the unnecessary spaces in the text (at the start and end)\n",
        "\n",
        "            #STOP CHECK\n",
        "            if text[-1] not in [\".\", \"!\", \"?\"]:  # check if the last character is a sentence-ending punctuation\n",
        "                #print(\"Sentence cut off, requesting continuation...\") #the response was cut-off mid-sentence due to token\n",
        "\n",
        "                continuation = api_request_with_retry(   # requesting a continuation to complete the last sentence\n",
        "                    model.generate_content,\n",
        "                    \"Continue from: \" + text[-50:],  # take the last 50 characters to ensure continuity\n",
        "                    generation_config={\"max_output_tokens\": min(MAX_TOKENS, remaining_tokens)}\n",
        "                )\n",
        "                if continuation:\n",
        "                    text += \" \" + continuation.text.strip()  # if continuation is received, append it to the response\n",
        "\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error generating text: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "nn5JgC7lgYsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Context Window - Chat Mode Method"
      ],
      "metadata": {
        "id": "8EVzNeDhqffX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history = []\n",
        "def chat_mode():\n",
        "    \"\"\"\n",
        "    Initiates an interactive chat session where user inputs are processed, and AI-generated responses\n",
        "    are displayed while maintaining conversation history.\n",
        "\n",
        "    The function ensures token limits are not exceeded and manages conversation history accordingly.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    global conversation_history\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Chat ended.\")\n",
        "            break\n",
        "\n",
        "        conversation_history.append(user_input)\n",
        "        context = \"\\n\".join(conversation_history)\n",
        "        input_tokens = count_tokens(context)\n",
        "        remaining_tokens = CONTEXT_WINDOW - input_tokens\n",
        "\n",
        "        if input_tokens >= CONTEXT_WINDOW:\n",
        "            print(\"⚠️ Warning: Context window limit exceeded! Consider clearing history.\")\n",
        "            print(\"🔄 Clearing the history...\")\n",
        "            conversation_history = []  # Clear the history to continue chatting\n",
        "            continue  # Skip the current iteration and start fresh\n",
        "\n",
        "        if input_tokens + MAX_TOKENS > CONTEXT_WINDOW * WARNING_THRESHOLD:\n",
        "            print(\"⚠️ Warning: Token usage is close to the limit!\")\n",
        "\n",
        "        try:\n",
        "            # Generate a response based on the current conversation context\n",
        "            response = api_request_with_retry(\n",
        "                model.generate_content,\n",
        "                context,\n",
        "                generation_config={\"max_output_tokens\": min(MAX_TOKENS, remaining_tokens)}\n",
        "            )\n",
        "\n",
        "            # If a response is successfully generated\n",
        "            if response:\n",
        "                response_text = response.text.strip()  # Remove any leading/trailing whitespace\n",
        "\n",
        "                #STOP CHECK\n",
        "                if response_text[-1] not in [\".\", \"!\", \"?\"]:\n",
        "                    # print(\"Chat response cut off, requesting continuation...\")\n",
        "\n",
        "\n",
        "                    continuation = api_request_with_retry(\n",
        "                        model.generate_content,\n",
        "                        \"Continue from: \" + response_text[-50:],  # maintain context with the last 50 characters\n",
        "                        generation_config={\"max_output_tokens\": min(MAX_TOKENS, remaining_tokens)}\n",
        "                    )\n",
        "\n",
        "\n",
        "                    if continuation:\n",
        "                        response_text += \" \" + continuation.text.strip()\n",
        "\n",
        "                # Add AI response to conversation history\n",
        "                conversation_history.append(response_text)\n",
        "\n",
        "                # Calculate token usage for the input and response\n",
        "                input_tokens, output_tokens, total_tokens = get_token_usage(context, response_text)\n",
        "\n",
        "                # Display token usage statistics\n",
        "                print(f\"📌 Input Tokens: {input_tokens}, Output Tokens: {output_tokens}, Total Tokens: {total_tokens}\")\n",
        "\n",
        "                # Print AI-generated response\n",
        "                print(f\"AI: {response_text}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle any errors that occur during text generation\n",
        "            print(f\"❌ Error generating response: {e}\")\n",
        "            continue  # Continue the chat loop despite the error"
      ],
      "metadata": {
        "id": "CxZkViIhiRQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_mode()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i9sYT7VgkKGN",
        "outputId": "797a8346-68d6-4e13-a656-88197877a338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: ağaç çeşitleri hakkında bilgi ver\n",
            "📌 Input Tokens: 8, Output Tokens: 499, Total Tokens: 507\n",
            "AI: Ağaçlar, dünyamızın ekosistemi için hayati öneme sahip, çok çeşitli türleri barındıran odunsu bitkilerdir.  İşte ağaç çeşitleri hakkında genel bir bakış ve bazı örnekler:\n",
            "\n",
            "**Genel Sınıflandırma:**\n",
            "\n",
            "* **Yaprakdöken Ağaçlar:** Sonbaharda yapraklarını döken ağaçlardır. Kışın soğuk ve kuru koşullarında su kaybını azaltmak için bu adaptasyonu geliştirmişlerdir.  Meşe, akçaağaç, kayın, huş, kavak, söğüt gibi ağaçlar bu gruba girer.\n",
            "\n",
            "* **Herdemyeşil Ağaçlar:** Yıl boyunca yeşil kalan ağaçlardır. Yapraklarını aynı anda dökmezler, sürekli olarak yeni yapraklar üretirler. Çam, köknar, ladin, sedir, servi gibi ağaçlar herdemyeşildir.\n",
            "\n",
            "* **Meyve Ağaçları:** Yenilebilir meyve veren ağaçlardır. Elma, armut, erik, şeftali, kiraz, kayısı, portakal, limon, mandalina gibi ağaçlar bu kategoriye girer.\n",
            "\n",
            "* **Süs Ağaçları:** Çiçekleri, yaprakları, kabukları veya genel görünümleriyle estetik değer taşıyan ağaçlardır.  Manolya, erguvan, zakkum, leylak, Japon akçaağacı gibi ağaçlar süs ağacı olarak yetiştirilir.\n",
            "\n",
            "**Bazı Önemli Ağaç Türleri ve Özellikleri:**\n",
            "\n",
            "* **Meşe (Quercus):** Sert ve dayanıklı odunu ile bilinen, birçok farklı türü olan bir ağaçtır.  Mobilya yapımında, parke üretiminde ve yakacak olarak kullanılır.\n",
            "\n",
            "* **Çam (Pinus):** İğne yapraklı, herdemyeşil bir ağaçtır. Reçine üretimi, kereste ve kağıt hamuru elde etmek için kullanılır.\n",
            "\n",
            "* **Kayın (Fagus):** Gölgelik alanlar oluşturan, düzgün gövdeli bir ağaçtır.  Mobilya, parke ve oyuncak yapımında kullanılır.\n",
            "\n",
            "* **Akçaağaç (Acer):** Şurup elde edilen türleri de olan, güzel sonbahar renkleriyle bilinen bir ağaçtır.\n",
            "You: meşe hangi coğrafyada bulunur \n",
            "⚠️ Warning: Token usage is close to the limit!\n",
            "📌 Input Tokens: 519, Output Tokens: 319, Total Tokens: 838\n",
            "AI: Meşe ağaçları oldukça geniş bir coğrafyaya yayılmıştır. Kuzey yarımkürede, özellikle ılıman bölgelerde yaygın olarak bulunurlar.  Daha detaylı inceleyecek olursak:\n",
            "\n",
            "* **Kuzey Amerika:** Meşe, Kuzey Amerika'da en yaygın ağaç türlerinden biridir.  Kanada'nın güneyinden Meksika'ya kadar geniş bir alanda, farklı meşe türleri bulunur.  Özellikle doğu kıyısı ve orta batı bölgeleri meşe ormanları açısından zengindir.\n",
            "\n",
            "* **Avrupa:** Avrupa'da da birçok meşe türü bulunur.  Akdeniz ülkelerinden İskandinavya'nın güneyine kadar geniş bir alanda yayılış gösterirler.\n",
            "\n",
            "* **Asya:** Asya'da, özellikle Çin, Japonya, Kore ve Hindistan gibi ülkelerde meşe ağaçları bulunur.  Ancak Asya'daki yayılışları Kuzey Amerika ve Avrupa'ya kıyasla daha sınırlıdır.\n",
            "\n",
            "* **Kuzey Afrika:** Kuzey Afrika'nın bazı bölgelerinde, özellikle Akdeniz kıyılarına yakın yerlerde meşe ağaçları bulunur.\n",
            "\n",
            "Kısacası, meşe ağaçları dünyanın birçok farklı bölgesinde bulunur, ancak en yoğun olarak Kuzey Amerika ve Avrupa'nın ılıman bölgelerinde görülürler.  Meşenin yayılışı, iklim koşulları ve toprak özelliklerine bağlı olarak değişiklik gösterir.\n",
            "You: meyve ağaçları hangi meyveleri verir\n",
            "⚠️ Warning: Token usage is close to the limit!\n",
            "📌 Input Tokens: 849, Output Tokens: 302, Total Tokens: 1151\n",
            "AI: Meyve ağaçları, çok çeşitli ve lezzetli meyveler verir. İşte bazı örnekler:\n",
            "\n",
            "**İklimimize Uygun Meyve Ağaçları ve Verdikleri Meyveler:**\n",
            "\n",
            "* **Elma:**  Çok sayıda çeşidi olan elma ağacı, farklı tat, renk ve dokularda elma verir.\n",
            "* **Armut:**  Elmaya benzer şekilde farklı çeşitleri olan armut ağacı, sulu ve tatlı armutlar verir.\n",
            "* **Erik:**  Mavi erik, yeşil erik, mürdüm eriği gibi farklı çeşitleri olan erik ağacı, çeşitli renk ve tatlarda erikler verir.\n",
            "* **Şeftali:**  Tüyl * **Şeftali:**  Tüylü kabuklu ve tatlı, sulu bir meyvedir. Çeşitli renklerde olabilirler, genellikle sarı, turuncu veya kırmızımsı tonlardadır. Çekirdekli bir meyvedir.\n",
            "* **Kayısı:**  Küçük, turuncu renkli ve tatlı bir meyvedir.  Şeftaliye benzer, ancak daha küçüktür ve tüylü bir kabuğu yoktur.  Çekirdekli bir meyvedir.\n",
            "* **Kiraz:**  Küçük, yuvarlak ve genellikle kırmızı, bazen de sarı veya siyah renkli bir meyvedir.  Tatlı veya ekşi çeşitleri vardır.  Çekirdek\n",
            "You: hi\n",
            "⚠️ Warning: Context window limit exceeded! Consider clearing history.\n",
            "🔄 Clearing the history...\n",
            "You: hi\n",
            "📌 Input Tokens: 1, Output Tokens: 10, Total Tokens: 11\n",
            "AI: Hi there! How can I help you today?\n",
            "You: exit\n",
            "Chat ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_mode()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "qItusdNBkNRL",
        "outputId": "207b8a8c-40a6-4e17-ae90-e3dc82fe06c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: hi\n",
            "📌 Input Tokens: 14, Output Tokens: 12, Total Tokens: 26\n",
            "AI: Hi!  Is there something I can assist you with?\n",
            "You: hi\n",
            "📌 Input Tokens: 29, Output Tokens: 19, Total Tokens: 48\n",
            "AI: Hello again!  Do you have a question or something you'd like me to do?\n",
            "You: hi\n",
            "📌 Input Tokens: 51, Output Tokens: 19, Total Tokens: 70\n",
            "AI: Hi there!  I'm still here.  Let me know if you need anything.\n",
            "You: hi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 253.93ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠ API Error Message: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).\n",
            "⚠️ Rate limit exceeded! Waiting for 2 seconds... (1/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 254.75ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Rate limit exceeded! Waiting for 4 seconds... (2/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 279.54ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Rate limit exceeded! Waiting for 8 seconds... (3/3)\n",
            "❌ Maximum number of retries reached. API request failed.\n",
            "You: hi\n",
            "📌 Input Tokens: 75, Output Tokens: 16, Total Tokens: 91\n",
            "AI: Hello!  Is there anything specific you want to talk about or ask me?\n",
            "You: exit\n",
            "Chat ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Request Testing (Text Generation)"
      ],
      "metadata": {
        "id": "pvHcbi6SrFxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(50):  # Simulating a long conversation\n",
        "    print(f\"Test Message {i+1}\")\n",
        "    response = generate_text(f\"This is message {i+1} in a long conversation.\")\n",
        "\n",
        "    if not response:\n",
        "        print(\"❌ Test Failed: No Response Generated!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "e_mEZh-kl_Uk",
        "outputId": "6afdaea4-0880-45b5-8009-2c01a94fc087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Message 1\n",
            "Test Message 2\n",
            "Test Message 3\n",
            "Test Message 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 254.33ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠ API Error Message: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).\n",
            "⚠️ Rate limit exceeded! Waiting for 2 seconds... (1/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 253.76ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Rate limit exceeded! Waiting for 4 seconds... (2/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 254.28ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Rate limit exceeded! Waiting for 8 seconds... (3/3)\n",
            "❌ Maximum number of retries reached. API request failed.\n",
            "❌ Test Failed: No Response Generated!\n"
          ]
        }
      ]
    }
  ]
}