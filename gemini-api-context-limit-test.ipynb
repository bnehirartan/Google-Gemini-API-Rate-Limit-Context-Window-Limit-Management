{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnehirartan/Google-Gemini-API-Rate-Limit-Context-Window-Limit-Management/blob/main/gemini-api-context-limit-test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Required Libraries"
      ],
      "metadata": {
        "id": "eYfjVH4qo4p6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1lUezp4XNd1"
      },
      "outputs": [],
      "source": [
        "#import numpy as np\n",
        "#from tqdm import tqdm\n",
        "#import pathlib\n",
        "import os\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_markdown(text): # function converts plain text from the LLM model to Markdown format, adding blockquote styling and converting bullet points.\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "ymh_7t_lXZt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use API Key and Generative AI Models"
      ],
      "metadata": {
        "id": "3meU2gSJrs4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GEMGEMINI_API_KEY') # using the API key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "Nt9c-0ogeYvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=genai.GenerativeModel('gemini-1.5-pro-latest') # Initialize the generative model with the latest Gemini-1.5-Pro version"
      ],
      "metadata": {
        "id": "g_5OrRaKesJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token Counting Methods"
      ],
      "metadata": {
        "id": "imzPIqZ5pZMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(text):\n",
        "    \"\"\"\n",
        "    Counts the number of tokens in a given text using the Gemini 1.5 Pro model.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The input text whose tokens need to be counted.\n",
        "\n",
        "    Returns:\n",
        "    int: The total number of tokens in the provided text.\n",
        "    \"\"\"\n",
        "    response = model.count_tokens(text) # Use the model's built-in token counting function to analyze the text\n",
        "    return response.total_tokens"
      ],
      "metadata": {
        "id": "SUenZPZJe3GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_token_usage(prompt, response_text):\n",
        "    \"\"\"\n",
        "    Calculates the token usage for a given prompt and response.\n",
        "\n",
        "    Parameters:\n",
        "    prompt (str): The input text provided to the model.\n",
        "    response_text (str): The generated response from the model.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the number of input tokens, output tokens, and total tokens used.\n",
        "    \"\"\"\n",
        "    input_tokens = count_tokens(prompt)\n",
        "    output_tokens = count_tokens(response_text)\n",
        "    total_tokens = input_tokens + output_tokens\n",
        "\n",
        "    return input_tokens, output_tokens, total_tokens"
      ],
      "metadata": {
        "id": "u9gfWlUdfM0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling API Rate Limit Method"
      ],
      "metadata": {
        "id": "VcpliA2Kp10G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use global constants instead of passing them as arguments\n",
        "MAX_TOKENS = 500\n",
        "CONTEXT_WINDOW = 1000\n",
        "WARNING_THRESHOLD = 0.8\n",
        "#RPM = 2\n",
        "#TPM = 32000"
      ],
      "metadata": {
        "id": "5RaUIi51fzee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def api_request_with_retry(request_func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Makes an API request with automatic retry logic in case of failures.\n",
        "\n",
        "    Parameters:\n",
        "    request_func (function): The API request function to be executed.\n",
        "    *args: Positional arguments to pass to the request function.\n",
        "    **kwargs: Keyword arguments to pass to the request function.\n",
        "\n",
        "    Returns:\n",
        "    Any: The response from the API request if successful, otherwise None.\n",
        "    \"\"\"\n",
        "    retries = 1\n",
        "    max_retries = 3\n",
        "    api_error_shown = False  # track if the error message was already printed (fixing the repeated API error message)*\n",
        "\n",
        "\n",
        "    while retries <= max_retries:  # Condition to allow 3 retries\n",
        "        try:\n",
        "            return request_func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            error_message = str(e).lower()\n",
        "\n",
        "            if \"429\" in error_message:  # Check for rate limit error\n",
        "                if not api_error_shown:\n",
        "                    print(f\"âš  API Error Message: {e}\")  # Print only once*\n",
        "                    api_error_shown = True\n",
        "\n",
        "                wait_time = 2 ** retries  # Exponential backoff logic\n",
        "                print(f\"âš ï¸ Rate limit exceeded! Waiting for {wait_time} seconds... ({retries}/{max_retries})\")\n",
        "                time.sleep(wait_time)\n",
        "                retries += 1\n",
        "            else:\n",
        "                # If it's not a rate limit error, print a general error message and exit\n",
        "                print(\"âŒ Error: API request is failed.\")\n",
        "                return None\n",
        "    # If we reach this point, all retries have failed\n",
        "    print(\"âŒ Maximum number of retries reached. API request failed.\")\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "v8gECN3of21E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Context Window - Text Generation Method"
      ],
      "metadata": {
        "id": "94g6WkmGqNSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt):\n",
        "    \"\"\"\n",
        "    Generates text based on the given prompt while handling token limits and ensuring sentence completeness.\n",
        "\n",
        "    Parameters:\n",
        "    prompt (str): The input text that serves as the basis for text generation.\n",
        "\n",
        "    Returns:\n",
        "    str or None: The generated text if successful, otherwise None.\n",
        "    \"\"\"\n",
        "\n",
        "    token_count = count_tokens(prompt)\n",
        "    remaining_tokens = CONTEXT_WINDOW - token_count #calculates how many tokens are left before hitting the context window limit\n",
        "\n",
        "    if token_count >= CONTEXT_WINDOW: # checking the number of token vs context window size\n",
        "        print(\"âš ï¸ Warning: Prompt exceeds context window limit!\")\n",
        "        return None\n",
        "\n",
        "    if token_count + MAX_TOKENS > CONTEXT_WINDOW * WARNING_THRESHOLD: # checking the warning THRESHOLD value\n",
        "        print(\"âš ï¸ Warning: Token usage is close to the limit! Consider shortening input.\")\n",
        "\n",
        "    try:\n",
        "        response = api_request_with_retry(model.generate_content, prompt, generation_config={\"max_output_tokens\": min(MAX_TOKENS, remaining_tokens)})\n",
        "        if response:\n",
        "            text = response.text.strip()#deletes the unnecessary spaces in the text (at the start and end)\n",
        "\n",
        "            #STOP CHECK\n",
        "            if text[-1] not in [\".\", \"!\", \"?\"]:  # check if the last character is a sentence-ending punctuation\n",
        "                #print(\"Sentence cut off, requesting continuation...\") #the response was cut-off mid-sentence due to token\n",
        "\n",
        "                continuation = api_request_with_retry(   # requesting a continuation to complete the last sentence\n",
        "                    model.generate_content,\n",
        "                    \"Continue from: \" + text[-50:],  # take the last 50 characters to ensure continuity\n",
        "                    generation_config={\"max_output_tokens\": min(MAX_TOKENS, remaining_tokens)}\n",
        "                )\n",
        "                if continuation:\n",
        "                    text += \" \" + continuation.text.strip()  # if continuation is received, append it to the response\n",
        "\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error generating text: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "nn5JgC7lgYsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Context Window - Chat Mode Method"
      ],
      "metadata": {
        "id": "8EVzNeDhqffX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history = []\n",
        "def chat_mode():\n",
        "    \"\"\"\n",
        "    Initiates an interactive chat session where user inputs are processed, and AI-generated responses\n",
        "    are displayed while maintaining conversation history.\n",
        "\n",
        "    The function ensures token limits are not exceeded and manages conversation history accordingly.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    global conversation_history\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Chat ended.\")\n",
        "            break\n",
        "\n",
        "        conversation_history.append(user_input)\n",
        "        context = \"\\n\".join(conversation_history)\n",
        "        input_tokens = count_tokens(context)\n",
        "        remaining_tokens = CONTEXT_WINDOW - input_tokens\n",
        "\n",
        "        if input_tokens >= CONTEXT_WINDOW:\n",
        "            print(\"âš ï¸ Warning: Context window limit exceeded! Consider clearing history.\")\n",
        "            print(\"ğŸ”„ Clearing the history...\")\n",
        "            conversation_history = []  # Clear the history to continue chatting\n",
        "            continue  # Skip the current iteration and start fresh\n",
        "\n",
        "        if input_tokens + MAX_TOKENS > CONTEXT_WINDOW * WARNING_THRESHOLD:\n",
        "            print(\"âš ï¸ Warning: Token usage is close to the limit!\")\n",
        "\n",
        "        try:\n",
        "            # Generate a response based on the current conversation context\n",
        "            response = api_request_with_retry(\n",
        "                model.generate_content,\n",
        "                context,\n",
        "                generation_config={\"max_output_tokens\": min(MAX_TOKENS, remaining_tokens)}\n",
        "            )\n",
        "\n",
        "            # If a response is successfully generated\n",
        "            if response:\n",
        "                response_text = response.text.strip()  # Remove any leading/trailing whitespace\n",
        "\n",
        "                #STOP CHECK\n",
        "                if response_text[-1] not in [\".\", \"!\", \"?\"]:\n",
        "                    # print(\"Chat response cut off, requesting continuation...\")\n",
        "\n",
        "\n",
        "                    continuation = api_request_with_retry(\n",
        "                        model.generate_content,\n",
        "                        \"Continue from: \" + response_text[-50:],  # maintain context with the last 50 characters\n",
        "                        generation_config={\"max_output_tokens\": min(MAX_TOKENS, remaining_tokens)}\n",
        "                    )\n",
        "\n",
        "\n",
        "                    if continuation:\n",
        "                        response_text += \" \" + continuation.text.strip()\n",
        "\n",
        "                # Add AI response to conversation history\n",
        "                conversation_history.append(response_text)\n",
        "\n",
        "                # Calculate token usage for the input and response\n",
        "                input_tokens, output_tokens, total_tokens = get_token_usage(context, response_text)\n",
        "\n",
        "                # Display token usage statistics\n",
        "                print(f\"ğŸ“Œ Input Tokens: {input_tokens}, Output Tokens: {output_tokens}, Total Tokens: {total_tokens}\")\n",
        "\n",
        "                # Print AI-generated response\n",
        "                print(f\"AI: {response_text}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle any errors that occur during text generation\n",
        "            print(f\"âŒ Error generating response: {e}\")\n",
        "            continue  # Continue the chat loop despite the error"
      ],
      "metadata": {
        "id": "CxZkViIhiRQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_mode()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i9sYT7VgkKGN",
        "outputId": "797a8346-68d6-4e13-a656-88197877a338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: aÄŸaÃ§ Ã§eÅŸitleri hakkÄ±nda bilgi ver\n",
            "ğŸ“Œ Input Tokens: 8, Output Tokens: 499, Total Tokens: 507\n",
            "AI: AÄŸaÃ§lar, dÃ¼nyamÄ±zÄ±n ekosistemi iÃ§in hayati Ã¶neme sahip, Ã§ok Ã§eÅŸitli tÃ¼rleri barÄ±ndÄ±ran odunsu bitkilerdir.  Ä°ÅŸte aÄŸaÃ§ Ã§eÅŸitleri hakkÄ±nda genel bir bakÄ±ÅŸ ve bazÄ± Ã¶rnekler:\n",
            "\n",
            "**Genel SÄ±nÄ±flandÄ±rma:**\n",
            "\n",
            "* **YaprakdÃ¶ken AÄŸaÃ§lar:** Sonbaharda yapraklarÄ±nÄ± dÃ¶ken aÄŸaÃ§lardÄ±r. KÄ±ÅŸÄ±n soÄŸuk ve kuru koÅŸullarÄ±nda su kaybÄ±nÄ± azaltmak iÃ§in bu adaptasyonu geliÅŸtirmiÅŸlerdir.  MeÅŸe, akÃ§aaÄŸaÃ§, kayÄ±n, huÅŸ, kavak, sÃ¶ÄŸÃ¼t gibi aÄŸaÃ§lar bu gruba girer.\n",
            "\n",
            "* **HerdemyeÅŸil AÄŸaÃ§lar:** YÄ±l boyunca yeÅŸil kalan aÄŸaÃ§lardÄ±r. YapraklarÄ±nÄ± aynÄ± anda dÃ¶kmezler, sÃ¼rekli olarak yeni yapraklar Ã¼retirler. Ã‡am, kÃ¶knar, ladin, sedir, servi gibi aÄŸaÃ§lar herdemyeÅŸildir.\n",
            "\n",
            "* **Meyve AÄŸaÃ§larÄ±:** Yenilebilir meyve veren aÄŸaÃ§lardÄ±r. Elma, armut, erik, ÅŸeftali, kiraz, kayÄ±sÄ±, portakal, limon, mandalina gibi aÄŸaÃ§lar bu kategoriye girer.\n",
            "\n",
            "* **SÃ¼s AÄŸaÃ§larÄ±:** Ã‡iÃ§ekleri, yapraklarÄ±, kabuklarÄ± veya genel gÃ¶rÃ¼nÃ¼mleriyle estetik deÄŸer taÅŸÄ±yan aÄŸaÃ§lardÄ±r.  Manolya, erguvan, zakkum, leylak, Japon akÃ§aaÄŸacÄ± gibi aÄŸaÃ§lar sÃ¼s aÄŸacÄ± olarak yetiÅŸtirilir.\n",
            "\n",
            "**BazÄ± Ã–nemli AÄŸaÃ§ TÃ¼rleri ve Ã–zellikleri:**\n",
            "\n",
            "* **MeÅŸe (Quercus):** Sert ve dayanÄ±klÄ± odunu ile bilinen, birÃ§ok farklÄ± tÃ¼rÃ¼ olan bir aÄŸaÃ§tÄ±r.  Mobilya yapÄ±mÄ±nda, parke Ã¼retiminde ve yakacak olarak kullanÄ±lÄ±r.\n",
            "\n",
            "* **Ã‡am (Pinus):** Ä°ÄŸne yapraklÄ±, herdemyeÅŸil bir aÄŸaÃ§tÄ±r. ReÃ§ine Ã¼retimi, kereste ve kaÄŸÄ±t hamuru elde etmek iÃ§in kullanÄ±lÄ±r.\n",
            "\n",
            "* **KayÄ±n (Fagus):** GÃ¶lgelik alanlar oluÅŸturan, dÃ¼zgÃ¼n gÃ¶vdeli bir aÄŸaÃ§tÄ±r.  Mobilya, parke ve oyuncak yapÄ±mÄ±nda kullanÄ±lÄ±r.\n",
            "\n",
            "* **AkÃ§aaÄŸaÃ§ (Acer):** Åurup elde edilen tÃ¼rleri de olan, gÃ¼zel sonbahar renkleriyle bilinen bir aÄŸaÃ§tÄ±r.\n",
            "You: meÅŸe hangi coÄŸrafyada bulunur \n",
            "âš ï¸ Warning: Token usage is close to the limit!\n",
            "ğŸ“Œ Input Tokens: 519, Output Tokens: 319, Total Tokens: 838\n",
            "AI: MeÅŸe aÄŸaÃ§larÄ± oldukÃ§a geniÅŸ bir coÄŸrafyaya yayÄ±lmÄ±ÅŸtÄ±r. Kuzey yarÄ±mkÃ¼rede, Ã¶zellikle Ä±lÄ±man bÃ¶lgelerde yaygÄ±n olarak bulunurlar.  Daha detaylÄ± inceleyecek olursak:\n",
            "\n",
            "* **Kuzey Amerika:** MeÅŸe, Kuzey Amerika'da en yaygÄ±n aÄŸaÃ§ tÃ¼rlerinden biridir.  Kanada'nÄ±n gÃ¼neyinden Meksika'ya kadar geniÅŸ bir alanda, farklÄ± meÅŸe tÃ¼rleri bulunur.  Ã–zellikle doÄŸu kÄ±yÄ±sÄ± ve orta batÄ± bÃ¶lgeleri meÅŸe ormanlarÄ± aÃ§Ä±sÄ±ndan zengindir.\n",
            "\n",
            "* **Avrupa:** Avrupa'da da birÃ§ok meÅŸe tÃ¼rÃ¼ bulunur.  Akdeniz Ã¼lkelerinden Ä°skandinavya'nÄ±n gÃ¼neyine kadar geniÅŸ bir alanda yayÄ±lÄ±ÅŸ gÃ¶sterirler.\n",
            "\n",
            "* **Asya:** Asya'da, Ã¶zellikle Ã‡in, Japonya, Kore ve Hindistan gibi Ã¼lkelerde meÅŸe aÄŸaÃ§larÄ± bulunur.  Ancak Asya'daki yayÄ±lÄ±ÅŸlarÄ± Kuzey Amerika ve Avrupa'ya kÄ±yasla daha sÄ±nÄ±rlÄ±dÄ±r.\n",
            "\n",
            "* **Kuzey Afrika:** Kuzey Afrika'nÄ±n bazÄ± bÃ¶lgelerinde, Ã¶zellikle Akdeniz kÄ±yÄ±larÄ±na yakÄ±n yerlerde meÅŸe aÄŸaÃ§larÄ± bulunur.\n",
            "\n",
            "KÄ±sacasÄ±, meÅŸe aÄŸaÃ§larÄ± dÃ¼nyanÄ±n birÃ§ok farklÄ± bÃ¶lgesinde bulunur, ancak en yoÄŸun olarak Kuzey Amerika ve Avrupa'nÄ±n Ä±lÄ±man bÃ¶lgelerinde gÃ¶rÃ¼lÃ¼rler.  MeÅŸenin yayÄ±lÄ±ÅŸÄ±, iklim koÅŸullarÄ± ve toprak Ã¶zelliklerine baÄŸlÄ± olarak deÄŸiÅŸiklik gÃ¶sterir.\n",
            "You: meyve aÄŸaÃ§larÄ± hangi meyveleri verir\n",
            "âš ï¸ Warning: Token usage is close to the limit!\n",
            "ğŸ“Œ Input Tokens: 849, Output Tokens: 302, Total Tokens: 1151\n",
            "AI: Meyve aÄŸaÃ§larÄ±, Ã§ok Ã§eÅŸitli ve lezzetli meyveler verir. Ä°ÅŸte bazÄ± Ã¶rnekler:\n",
            "\n",
            "**Ä°klimimize Uygun Meyve AÄŸaÃ§larÄ± ve Verdikleri Meyveler:**\n",
            "\n",
            "* **Elma:**  Ã‡ok sayÄ±da Ã§eÅŸidi olan elma aÄŸacÄ±, farklÄ± tat, renk ve dokularda elma verir.\n",
            "* **Armut:**  Elmaya benzer ÅŸekilde farklÄ± Ã§eÅŸitleri olan armut aÄŸacÄ±, sulu ve tatlÄ± armutlar verir.\n",
            "* **Erik:**  Mavi erik, yeÅŸil erik, mÃ¼rdÃ¼m eriÄŸi gibi farklÄ± Ã§eÅŸitleri olan erik aÄŸacÄ±, Ã§eÅŸitli renk ve tatlarda erikler verir.\n",
            "* **Åeftali:**  TÃ¼yl * **Åeftali:**  TÃ¼ylÃ¼ kabuklu ve tatlÄ±, sulu bir meyvedir. Ã‡eÅŸitli renklerde olabilirler, genellikle sarÄ±, turuncu veya kÄ±rmÄ±zÄ±msÄ± tonlardadÄ±r. Ã‡ekirdekli bir meyvedir.\n",
            "* **KayÄ±sÄ±:**  KÃ¼Ã§Ã¼k, turuncu renkli ve tatlÄ± bir meyvedir.  Åeftaliye benzer, ancak daha kÃ¼Ã§Ã¼ktÃ¼r ve tÃ¼ylÃ¼ bir kabuÄŸu yoktur.  Ã‡ekirdekli bir meyvedir.\n",
            "* **Kiraz:**  KÃ¼Ã§Ã¼k, yuvarlak ve genellikle kÄ±rmÄ±zÄ±, bazen de sarÄ± veya siyah renkli bir meyvedir.  TatlÄ± veya ekÅŸi Ã§eÅŸitleri vardÄ±r.  Ã‡ekirdek\n",
            "You: hi\n",
            "âš ï¸ Warning: Context window limit exceeded! Consider clearing history.\n",
            "ğŸ”„ Clearing the history...\n",
            "You: hi\n",
            "ğŸ“Œ Input Tokens: 1, Output Tokens: 10, Total Tokens: 11\n",
            "AI: Hi there! How can I help you today?\n",
            "You: exit\n",
            "Chat ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_mode()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "qItusdNBkNRL",
        "outputId": "207b8a8c-40a6-4e17-ae90-e3dc82fe06c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: hi\n",
            "ğŸ“Œ Input Tokens: 14, Output Tokens: 12, Total Tokens: 26\n",
            "AI: Hi!  Is there something I can assist you with?\n",
            "You: hi\n",
            "ğŸ“Œ Input Tokens: 29, Output Tokens: 19, Total Tokens: 48\n",
            "AI: Hello again!  Do you have a question or something you'd like me to do?\n",
            "You: hi\n",
            "ğŸ“Œ Input Tokens: 51, Output Tokens: 19, Total Tokens: 70\n",
            "AI: Hi there!  I'm still here.  Let me know if you need anything.\n",
            "You: hi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 253.93ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš  API Error Message: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).\n",
            "âš ï¸ Rate limit exceeded! Waiting for 2 seconds... (1/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 254.75ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ Rate limit exceeded! Waiting for 4 seconds... (2/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 279.54ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ Rate limit exceeded! Waiting for 8 seconds... (3/3)\n",
            "âŒ Maximum number of retries reached. API request failed.\n",
            "You: hi\n",
            "ğŸ“Œ Input Tokens: 75, Output Tokens: 16, Total Tokens: 91\n",
            "AI: Hello!  Is there anything specific you want to talk about or ask me?\n",
            "You: exit\n",
            "Chat ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Request Testing (Text Generation)"
      ],
      "metadata": {
        "id": "pvHcbi6SrFxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(50):  # Simulating a long conversation\n",
        "    print(f\"Test Message {i+1}\")\n",
        "    response = generate_text(f\"This is message {i+1} in a long conversation.\")\n",
        "\n",
        "    if not response:\n",
        "        print(\"âŒ Test Failed: No Response Generated!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "e_mEZh-kl_Uk",
        "outputId": "6afdaea4-0880-45b5-8009-2c01a94fc087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Message 1\n",
            "Test Message 2\n",
            "Test Message 3\n",
            "Test Message 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 254.33ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš  API Error Message: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).\n",
            "âš ï¸ Rate limit exceeded! Waiting for 2 seconds... (1/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 253.76ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ Rate limit exceeded! Waiting for 4 seconds... (2/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 254.28ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ Rate limit exceeded! Waiting for 8 seconds... (3/3)\n",
            "âŒ Maximum number of retries reached. API request failed.\n",
            "âŒ Test Failed: No Response Generated!\n"
          ]
        }
      ]
    }
  ]
}